{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Construction\n",
    "\n",
    "In this notebook, we construct the data set that will be basis for our experiments. As mentioned in the paper, we are working with the medical domain, more specifically the relations between medical conditions (diseases) and treatments (drugs). We query [Wikidata's SPARQL endpoints](https://query.wikidata.org) using the [SPARQLWrapper library](https://rdflib.github.io/sparqlwrapper/) and process the data with a combintation of [rdflib](https://rdflib.readthedocs.io/en/stable/) and [networkx](http://networkx.github.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib.graph\n",
    "import networkx as nx\n",
    "import os.path\n",
    "import hashlib\n",
    "import json\n",
    "import SPARQLWrapper\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we set up a connection to the SPARQL endpoint that we will use throughout. Furhtermore, the helper function `query` executes a SPARQL query, converts it to an `rdflib` graph and caches the result. It it only possible to execute a [limitted number of queries to Wikidata](https://www.mediawiki.org/wiki/Wikidata_Query_Service/User_Manual#Query_limits) so this caching functionality saves as many requests to the endpoint as possible (with an invalidation period of one day)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparql = SPARQLWrapper.SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "\n",
    "cache_dir = \"_query_cache\"\n",
    "cache_dict = f\"{cache_dir}/_dict.json\"\n",
    "\n",
    "cache_invalidation_periode = dt.timedelta(days=1)\n",
    "\n",
    "\n",
    "def query(query: str, force=False) -> rdflib.Graph:\n",
    "    # Ensure that the cache directory exists.\n",
    "    if not os.path.isfile(cache_dict):\n",
    "        os.makedirs(cache_dir)\n",
    "        json.dump({}, open(cache_dict, \"w\"))\n",
    "\n",
    "    # Load the cache.\n",
    "    cache = json.load(open(cache_dict))\n",
    "\n",
    "    # The MD5 of the query acts as the cache key.\n",
    "    key = hashlib.md5(query.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "    # Is the key is present in the cache, load and return the cached data.\n",
    "    if not force and key in cache:\n",
    "        cache_entry = cache[key]\n",
    "        created_at = dt.datetime.fromtimestamp(cache_entry[\"timestamp\"])\n",
    "\n",
    "        # If the cache entry is not invalidated, return the cached data.\n",
    "        if dt.datetime.now() < created_at + cache_invalidation_periode:\n",
    "            return rdflib.graph.Graph().parse(cache_entry[\"path\"])\n",
    "\n",
    "    # Execute SPARLQL query.\n",
    "    sparql.setQuery(query)\n",
    "    result = sparql.queryAndConvert()\n",
    "\n",
    "    # Save the response to the cache.\n",
    "    dest = f\"{cache_dir}/{key}.xml\"\n",
    "    result.serialize(dest)\n",
    "    cache[key] = {\"timestamp\": dt.datetime.now().timestamp(), \"path\": dest}\n",
    "    json.dump(cache, open(cache_dict, \"w\"))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct our data as a directed graph using the networkx class `DiGraph`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.DiGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diseases\n",
    "\n",
    "The following two queries fetch [instances (P31)](https://www.wikidata.org/wiki/Property:P31) and [subclasses (P279)](https://www.wikidata.org/wiki/Property:P279) of the [disease (Q12136)](https://www.wikidata.org/wiki/Q12136) entity. This will give the graph a hierarcical structure that suits the HAKE link prediction technique. First, we fetch entites that are direct disease instances and add them to the graph with a \"is-a\" edge to the \"disease\" node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_instances_query = \"\"\"\n",
    "    CONSTRUCT\n",
    "    WHERE {\n",
    "        ?d wdt:P31 wd:Q12136.\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "disease_instances_result = query(disease_instances_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subj, obj in disease_instances_result.subject_objects():\n",
    "    graph.add_edge(subj, \"disease\", type=\"is-a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we get all instances of entites that are subclasses of the dieases entity. These are added to the graph with a \"is-a\" edge to the subclass which in turn has a \"subclass-of\" edge to the \"disease\" node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_subclasses_query = \"\"\"\n",
    "    CONSTRUCT {\n",
    "      ?disease wdt:P31 ?subclass.\n",
    "    }\n",
    "    WHERE {\n",
    "      ?subclass wdt:P279 wd:Q12136.\n",
    "      ?disease wdt:P31 ?subclass.\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "disease_subclasses_result = query(disease_subclasses_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subj, obj in disease_subclasses_result.subject_objects():\n",
    "    graph.add_edge(subj, obj, type=\"is-a\")\n",
    "    graph.add_edge(obj, \"disease\", type=\"subclass-of\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medication\n",
    "\n",
    "In the same way as above, we create a subclass-instance hierarchy for medications based on the [medication (Q12140)](https://www.wikidata.org/wiki/Q12140) entity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "medication_instances_query = \"\"\"\n",
    "    CONSTRUCT\n",
    "    WHERE {\n",
    "        ?d wdt:P31 wd:Q12140.\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "medication_instances_result = query(medication_instances_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subj, obj in medication_instances_result.subject_objects():\n",
    "    graph.add_edge(subj, \"meddication\", type=\"is-a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "medication_subclasses_query = \"\"\"\n",
    "    CONSTRUCT {\n",
    "      ?disease wdt:P31 ?subclass.\n",
    "    }\n",
    "    WHERE {\n",
    "      ?subclass wdt:P279 wd:Q12140.\n",
    "      ?disease wdt:P31 ?subclass.\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "medication_subclasses_result = query(medication_subclasses_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subj, obj in medication_subclasses_result.subject_objects():\n",
    "    graph.add_edge(subj, obj, type=\"is-a\")\n",
    "    graph.add_edge(obj, \"medication\", type=\"subclass-of\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treaments\n",
    "\n",
    "Now we add the relations between diseases and medications starting with the [drug used for treatment (P2176)](https://www.wikidata.org/wiki/Property:P2176) predicate. If, for some reason, we encounter a medication or disease that is not present in the graph, we ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatments_query = \"\"\"\n",
    "    CONSTRUCT \n",
    "    WHERE {\n",
    "      ?disease wdt:P2176 ?medication.\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "treatments_result = query(treatments_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subj, obj in treatments_result.subject_objects():\n",
    "    if subj not in graph.nodes() or obj not in graph.nodes():\n",
    "        continue\n",
    "\n",
    "    graph.add_edge(subj, obj, type=\"treated-with\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symptoms\n",
    "\n",
    "We repeat the process above for symptoms querying for the [symptoms (P780)](https://www.wikidata.org/wiki/Property:P780) predicate. Since this predicate is not strictly related to diseases in Wikidata, we skip the triples where the subject is not already in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "symptoms_query = \"\"\"\n",
    "    CONSTRUCT\n",
    "    WHERE {\n",
    "        ?d wdt:P780 ?s.\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "symptoms_result = query(symptoms_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subj, obj in symptoms_result.subject_objects():\n",
    "    if subj not in graph.nodes():\n",
    "        continue\n",
    "\n",
    "    graph.add_edge(subj, obj, type=\"has-symptom\")\n",
    "    graph.add_edge(obj, \"symptom\", type=\"is-a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causes\n",
    "\n",
    "Using the [has cause (P828)](https://www.wikidata.org/wiki/Property:P828) predicate, we do the same as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "causes_query = \"\"\"\n",
    "    CONSTRUCT\n",
    "    WHERE {\n",
    "        ?d wdt:P828 ?s.\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "causes_result = query(causes_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is added to the graph in the same way as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subj, obj in causes_result.subject_objects():\n",
    "    if subj not in graph.nodes():\n",
    "        continue\n",
    "\n",
    "    graph.add_edge(subj, obj, type=\"has-cause\")\n",
    "    graph.add_edge(obj, \"cause\", type=\"is-a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20422 nodes\n",
      "34216 edges\n",
      "1001 has-cause edges\n",
      "26676 is-a edges\n",
      "1433 has-symptom edges\n",
      "87 subclass-of edges\n",
      "5019 treated-with edges\n"
     ]
    }
   ],
   "source": [
    "print(f\"{graph.number_of_nodes()} nodes\")\n",
    "    \n",
    "print(f\"{graph.number_of_edges()} edges\")\n",
    "\n",
    "for type in set(nx.get_edge_attributes(graph, 'type').values()):\n",
    "    number = len(\n",
    "        [(head, tail) for head, tail, data in graph.edges(data=True) if data[\"type\"] == type]\n",
    "    )\n",
    "    print(f\"{number} {type} edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can save the graph to the file system and load it up elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gml(graph, \"data.gml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion to a HAKE-Friendly Format\n",
    "\n",
    "The HAKE link prediction model expects a certain data format. First of all, the data must be represented as `(head, relation, tail)` triples and be split up into a training, validation, and testing dataset. Second, the entities and relations must be defined in a dictionary structure that maps ther string representation to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.read_gml(\"data.gml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = set(graph.nodes())\n",
    "entity_dict = {entity: n for n, entity in enumerate(entities)}\n",
    "\n",
    "relations = set([data[\"type\"] for _head, _tail, data in graph.edges(data=True)])\n",
    "relation_dict = {relation: n for n, relation in enumerate(relations)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "triples = [(head, data[\"type\"], tail) for head, tail, data in graph.edges(data=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to split the data, we define a proportion of the triples to be used for validation and testing data. The remaining triples will be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_proportion = 0.1\n",
    "testing_proportion = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set_size = round(len(triples) * validation_proportion)\n",
    "test_set_size = round(len(triples) * testing_proportion)\n",
    "training_set_size = len(triples) - validation_set_size - test_set_size\n",
    "\n",
    "training_set = triples[:training_set_size]\n",
    "validation_set = triples[training_set_size:training_set_size + validation_set_size]\n",
    "test_set = triples[training_set_size + validation_set_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we write the entity and relation dictionaries as well as the datasets to disk. The resulting files can be passed directly into the HAKE model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"hake_data\"\n",
    "\n",
    "if not os.path.isdir(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "def write_triples(triples, file):\n",
    "    with open(file, \"w\") as f:\n",
    "        for head, relation, tail in triples:\n",
    "            string = \"{}\\t{}\\t{}\\n\".format(head, relation, tail)\n",
    "            f.write(string)\n",
    "\n",
    "def write_dict(dictionary, file):\n",
    "    with open(file, \"w\") as f:\n",
    "        for key, value in dictionary.items():\n",
    "            string = \"{}\\t{}\\n\".format(value, key)\n",
    "            f.write(string)\n",
    "    \n",
    "write_triples(training_set, f\"{directory}/train.txt\")\n",
    "write_triples(validation_set, f\"{directory}/valid.txt\")\n",
    "write_triples(test_set, f\"{directory}/test.txt\")\n",
    "write_dict(entity_dict, f\"{directory}/entities.dict\")\n",
    "write_dict(relation_dict, f\"{directory}/relations.dict\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
